{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82f39e94",
   "metadata": {},
   "source": [
    "# How can we determine that model has overfit or underfit ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6379814e",
   "metadata": {},
   "source": [
    "1. Plotting the learning curve: A learning curve plots the model's performance on the training and validation sets as a function  of the number of training examples. If the model is overfitting, the training set performance will be much higher than the validation set performance. If the model is underfitting, the performance on both sets will be low.\n",
    "\n",
    "2. Comparing the performance on training and validation sets: If the model is overfitting, it will perform well on the training set but poorly on the validation set. If the model is underfitting, it will perform poorly on both sets.\n",
    "\n",
    "3. Using techniques such as cross-validation: Cross-validation is a technique that involves dividing the data into multiple folds and training and evaluating the model on different subsets of the data. This gives a more robust estimate of the model's performance on unseen data.\n",
    "\n",
    "4. Using Regularization: Regularization technique is used to prevent overfitting by adding a penalty term to the loss function, which discourages large weights.\n",
    "\n",
    "5. Early stopping: One can monitor the performance of the model on a validation set during training. If the performance on the validation set stops improving or starts to degrade, training can be stopped early to prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "29ac43ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error \n",
    "from sklearn.metrics import mean_squared_error \n",
    "from sklearn.metrics import median_absolute_error\n",
    "from sklearn.metrics import r2_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a08e1ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating some data\n",
    "np.random.seed(0)\n",
    "X, y = np.random.rand(100, 1), np.random.rand(100, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "621dcb5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data into training and validation sets\n",
    "X_train, X_test, y_train, y_test= train_test_split(X,y ,test_size=0.30 ,  random_state=44 , shuffle =True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cc8a4f2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training the model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "06f71ab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression Train Score is :  0.0518553356782635\n"
     ]
    }
   ],
   "source": [
    "print('Linear Regression Train Score is : ' , model.score(X_train, y_train)) #evaluate the performance of a linear regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "33849b12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression Test Score is :  -0.5857858401549687\n"
     ]
    }
   ],
   "source": [
    "print('Linear Regression Test Score is : ' , model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b3c5b87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the model on the training set\n",
    "train= model.score(X_train, y_train)\n",
    "\n",
    "# Evaluating the model on the validation set\n",
    "test = model.score(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1ed09b1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is overfitting\n"
     ]
    }
   ],
   "source": [
    "# Check for overfitting or underfitting\n",
    "if train > test:\n",
    "    print(\"Model is overfitting\")\n",
    "elif train < test:\n",
    "    print(\"Model is underfitting\")\n",
    "else:\n",
    "    print(\"Model is neither overfitting nor underfitting\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
