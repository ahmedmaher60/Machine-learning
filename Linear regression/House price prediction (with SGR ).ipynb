{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "281c61b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "44d8fb50",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('C:\\\\Users\\\\AHMED\\\\Documents\\\\Machine Learning\\\\SKlearn_files-master\\\\Data\\\\2.1 Linear Regression\\\\houses.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e13d7579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 18 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   bedrooms       999 non-null    float64\n",
      " 1   bathrooms      999 non-null    float64\n",
      " 2   sqft_living    999 non-null    float64\n",
      " 3   sqft_lot       999 non-null    float64\n",
      " 4   floors         999 non-null    float64\n",
      " 5   waterfront     999 non-null    float64\n",
      " 6   view           999 non-null    float64\n",
      " 7   condition      999 non-null    float64\n",
      " 8   grade          999 non-null    float64\n",
      " 9   sqft_above     999 non-null    float64\n",
      " 10  sqft_basement  999 non-null    float64\n",
      " 11  yr_built       999 non-null    float64\n",
      " 12  yr_renovated   999 non-null    float64\n",
      " 13  zipcode        999 non-null    float64\n",
      " 14  lat            999 non-null    float64\n",
      " 15  long           999 non-null    float64\n",
      " 16  sqft_living15  999 non-null    float64\n",
      " 17  price          999 non-null    float64\n",
      "dtypes: float64(18)\n",
      "memory usage: 140.8 KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9168e8e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>sqft_living</th>\n",
       "      <th>sqft_lot</th>\n",
       "      <th>floors</th>\n",
       "      <th>waterfront</th>\n",
       "      <th>view</th>\n",
       "      <th>condition</th>\n",
       "      <th>grade</th>\n",
       "      <th>sqft_above</th>\n",
       "      <th>sqft_basement</th>\n",
       "      <th>yr_built</th>\n",
       "      <th>yr_renovated</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>sqft_living15</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     bedrooms  bathrooms  sqft_living  sqft_lot  floors  waterfront   view  \\\n",
       "0       False      False        False     False   False       False  False   \n",
       "1       False      False        False     False   False       False  False   \n",
       "2       False      False        False     False   False       False  False   \n",
       "3       False      False        False     False   False       False  False   \n",
       "4       False      False        False     False   False       False  False   \n",
       "..        ...        ...          ...       ...     ...         ...    ...   \n",
       "995     False      False        False     False   False       False  False   \n",
       "996     False      False        False     False   False       False  False   \n",
       "997     False      False        False     False   False       False  False   \n",
       "998     False      False        False     False   False       False  False   \n",
       "999      True       True         True      True    True        True   True   \n",
       "\n",
       "     condition  grade  sqft_above  sqft_basement  yr_built  yr_renovated  \\\n",
       "0        False  False       False          False     False         False   \n",
       "1        False  False       False          False     False         False   \n",
       "2        False  False       False          False     False         False   \n",
       "3        False  False       False          False     False         False   \n",
       "4        False  False       False          False     False         False   \n",
       "..         ...    ...         ...            ...       ...           ...   \n",
       "995      False  False       False          False     False         False   \n",
       "996      False  False       False          False     False         False   \n",
       "997      False  False       False          False     False         False   \n",
       "998      False  False       False          False     False         False   \n",
       "999       True   True        True           True      True          True   \n",
       "\n",
       "     zipcode    lat   long  sqft_living15  price  \n",
       "0      False  False  False          False  False  \n",
       "1      False  False  False          False  False  \n",
       "2      False  False  False          False  False  \n",
       "3      False  False  False          False  False  \n",
       "4      False  False  False          False  False  \n",
       "..       ...    ...    ...            ...    ...  \n",
       "995    False  False  False          False  False  \n",
       "996    False  False  False          False  False  \n",
       "997    False  False  False          False  False  \n",
       "998    False  False  False          False  False  \n",
       "999     True   True   True           True   True  \n",
       "\n",
       "[1000 rows x 18 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3399cb0c",
   "metadata": {},
   "source": [
    "# Data cleaning with sklearn library\n",
    "### SimpleImputer is a class in the scikit-learn library that can be used to handle missing values in a dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24b1494",
   "metadata": {},
   "source": [
    "> impute.SimpleImputer(missing_values=nan, strategy='mean’, fill_value=None, verbose=0, copy=True)\n",
    "## parameter: - \n",
    "1. The missing_values parameter specifies which values in the dataset should be considered as missing. The default value is \"nan\", which stands for \"Not a Number\".\n",
    "\n",
    "2. The strategy parameter specifies how the missing values should be filled. The 'mean' strategy fills the missing values with the mean value of the column.\n",
    "\n",
    "3. The fill_value parameter allows to specify a custom value to fill the missing values. If fill_value is None, the mean value is used instead.\n",
    "\n",
    "4. The verbose parameter controls the amount of information displayed during the imputation process.\n",
    "\n",
    "5. The copy parameter, if set to True, makes a copy of the data before imputing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b73a07b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f8016be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ImputedModule = SimpleImputer(missing_values = np.nan, strategy ='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fbed6b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "ImputedModule = ImputedModule.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ef8c9c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ImputedModule.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "399a13d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[:, :-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b7aedb51",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df[:, -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff42c17",
   "metadata": {},
   "source": [
    "# Rescaling, also known as normalization or standardization, is the process of transforming a variable to have a values between a specific range, typically between 0 and 1. It is a preprocessing technique that is commonly used in machine learning and other data analysis tasks to ensure that all variables are on the same scale and to prevent variables with larger scales from dominating the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f1d91b",
   "metadata": {},
   "source": [
    "# There are several ways to determine if data needs to be rescaled:\n",
    "\n",
    "1. Visual inspection: A simple way to check if data needs to be rescaled is by visualizing the data using histograms or box plots. If the data for a variable is not normally distributed or if the range of values is significantly different across variables, it may be necessary to rescale the data.\n",
    "\n",
    "2. Range of values: If the range of values for a variable is significantly different from other variables, it may be necessary to rescale the data. For example, if one variable has values ranging from 0 to 100,000 and another variable has values ranging from 0 to 1, it may be necessary to rescale the data so that the variables are on the same scale.\n",
    "\n",
    "3. Algorithm performance: If a model's performance is poor and you suspect that the data might not be normalized, it could be a sign that data needs to be rescaled.\n",
    "\n",
    "4. Machine learning models sensitivity: Some models like Neural Network, K-Nearest Neighbors (KNN) and Support Vector Machine (SVM) are sensitive to the scale of the features, if you are using one of these models it could be a sign that data needs to be rescaled.\n",
    "\n",
    "It's worth noting that rescaling is not always necessary and the necessity of rescaling the data depends on the specific dataset and the analysis you want to perform."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71cffe4",
   "metadata": {},
   "source": [
    "# There are several ways to rescale data, including:\n",
    "\n",
    "1. Min-Max scaling: This method scales the data so that the minimum value of a variable is 0 and the maximum value is 1. This can be done by subtracting the minimum value from each data point and then dividing by the range (max-min) of the variable.\n",
    "\n",
    "2. Z-score normalization: This method scales the data so that the mean of the variable is 0 and the standard deviation is 1. This can be done by subtracting the mean from each data point and then dividing by the standard deviation.\n",
    "\n",
    "3. You can use other preprocessing functions in scikit-learn such as StandardScaler() , RobustScaler() and MinMaxScaler which can handle outliers.."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b19016",
   "metadata": {},
   "source": [
    "# Data scaling with sklearn library Standardscaler \n",
    "## it is used to rescale data between 0 - 1 \n",
    "###  Standardscaler = X-XMEAN/STD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1158371d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "02e8b1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "53de857a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sc.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "eb0ca1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split data\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2937ea79",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test= train_test_split(X,y ,test_size=0.30 ,  random_state=44 , shuffle =True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ac76ddc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.76410982, -1.44995485, -0.33961712, ...,  0.18512719,\n",
       "         0.50498   , -0.3534258 ],\n",
       "       [-0.4102682 , -0.41010927, -0.32834904, ..., -1.30833131,\n",
       "         1.12897521, -0.3385016 ],\n",
       "       [-1.58464621, -1.44995485, -1.13965109, ...,  1.13833839,\n",
       "        -1.03707712, -0.81607618],\n",
       "       ...,\n",
       "       [-1.58464621,  0.62973631,  0.37027218, ...,  1.18288758,\n",
       "         1.34414597, -0.17433534],\n",
       "       [ 1.93848783,  3.05604267,  4.5281952 , ...,  0.32301754,\n",
       "         1.84621108,  4.01936643],\n",
       "       [-0.4102682 , -0.75672446, -0.12552352, ..., -1.56148384,\n",
       "        -0.85776815, -0.86084879]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1da7d673",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.76410982, -0.06349407, -0.54244263, ..., -0.376334  ,\n",
       "        -0.68563154, -0.15941113],\n",
       "       [-0.4102682 , -1.44995485, -0.90302132, ..., -1.52329882,\n",
       "        -0.70714862, -1.05486347],\n",
       "       [ 0.76410982,  0.28312112,  2.44359964, ..., -0.74050594,\n",
       "        -1.00121533,  0.63157176],\n",
       "       ...,\n",
       "       [-0.4102682 , -0.41010927, -0.66639156, ..., -0.40815485,\n",
       "        -0.78604456, -1.1444087 ],\n",
       "       [-0.4102682 , -1.44995485, -0.90302132, ..., -0.52836695,\n",
       "         0.69863369, -0.80115197],\n",
       "       [-1.58464621, -1.44995485, -1.21852768, ...,  0.01117322,\n",
       "        -0.47046078, -0.74145515]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e00ff116",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 42.        ,  24.        ,  43.6       ,  55.7       ,\n",
       "        42.5       ,  46.5       ,  45.5       ,  38.5       ,\n",
       "        53.99      ,  55.225     ,  42.        ,  52.425     ,\n",
       "        44.65      ,  26.5       ,  59.5       ,  43.8       ,\n",
       "       132.        ,  35.        ,  26.5       ,  63.        ,\n",
       "        18.025     ,  30.9       ,  59.5       ,  45.2       ,\n",
       "        31.3       ,  35.5       ,  31.5       , 110.        ,\n",
       "        32.        ,  32.5       ,  46.        ,  55.        ,\n",
       "        19.9       ,  55.35      ,  25.75      ,  33.        ,\n",
       "        72.5       ,  98.        ,  37.        ,  26.        ,\n",
       "        66.05      ,  33.1       ,  73.        ,  30.5       ,\n",
       "        79.5       ,  73.75      ,  60.        ,  26.995     ,\n",
       "        65.        ,  40.5       ,  33.35      ,  46.        ,\n",
       "        21.        ,  39.        ,  42.67      , 148.        ,\n",
       "       112.        ,  49.5       ,  57.        ,  26.        ,\n",
       "        46.1       ,  37.9       ,  52.        , 105.        ,\n",
       "        86.3       , 135.        ,  29.        ,  46.7       ,\n",
       "        39.        ,  52.1       ,  13.        ,  23.2       ,\n",
       "        37.5       ,  61.5       ,  93.        ,  85.5       ,\n",
       "        49.65      ,  32.3       ,  39.75      ,  68.5       ,\n",
       "        29.9995    ,  38.        ,  28.3       ,  32.45      ,\n",
       "        26.5       ,  37.        ,  55.995     ,  33.1       ,\n",
       "        24.99      ,  36.        ,  65.        ,  46.5       ,\n",
       "        25.3       ,  74.        ,  28.        ,  53.        ,\n",
       "        45.        ,  69.995     ,  45.2       ,  33.2       ,\n",
       "        39.995     ,  82.4       , 158.        ,  28.        ,\n",
       "        31.2       , 138.        ,  46.        ,  31.5       ,\n",
       "        28.9999    ,  27.25      ,  29.        ,  42.5       ,\n",
       "        67.5       ,  59.775     ,  43.75      ,  73.        ,\n",
       "        46.525     ,  64.1       ,  23.8       ,  62.5       ,\n",
       "        23.5       ,  32.8       ,  35.2       ,  40.55      ,\n",
       "        32.        ,  40.        ,  37.        ,  23.2       ,\n",
       "        29.25      ,  81.        ,  24.2       ,  60.99      ,\n",
       "        53.        ,  48.        ,  20.5       ,  21.2       ,\n",
       "        45.        ,  32.995     ,  66.7       ,  54.9       ,\n",
       "        25.5       ,  62.5       ,  57.6       ,  45.3246    ,\n",
       "        43.5       ,  98.5       ,  48.8       ,  69.3       ,\n",
       "        23.6       ,  72.        ,  30.96      ,  39.9       ,\n",
       "        43.8       , 133.        ,  38.        , 290.        ,\n",
       "        20.5       ,  30.        ,  53.        ,  31.        ,\n",
       "       104.        ,  38.9       ,  28.        ,  54.5       ,\n",
       "        92.        ,  36.5       ,  23.75      , 140.        ,\n",
       "        90.        ,  77.        , 151.        ,  75.        ,\n",
       "        36.5       ,  31.        ,  45.5       ,  45.75      ,\n",
       "        25.6883    ,  59.5       ,  57.8       ,  53.5       ,\n",
       "        33.        ,  21.        ,  34.55      ,  68.5       ,\n",
       "        70.5       ,  30.49      ,  68.4       ,  46.6       ,\n",
       "        53.217     ,  48.        ,  68.6       , 205.        ,\n",
       "        50.7       ,  27.25      ,  87.9       ,  18.8       ,\n",
       "        67.3       ,  42.5       ,  46.        ,  90.        ,\n",
       "        26.5       ,  55.        ,  65.        ,  72.5       ,\n",
       "        28.        ,  53.58      ,  61.9       ,  43.        ,\n",
       "       136.        ,  57.75      ,  48.        ,  39.5       ,\n",
       "        83.99      ,  46.575     , 103.        ,  21.        ,\n",
       "        74.05      ,  46.5       , 308.        ,  36.5       ,\n",
       "        57.7       ,  23.7       ,  42.995     ,  45.        ,\n",
       "        89.        ,  31.7625    ,  42.5       ,  89.1       ,\n",
       "        35.        ,  51.1       ,  29.7       ,  26.4       ,\n",
       "        24.05      ,  97.1       ,  37.05      ,  64.5       ,\n",
       "        26.875     ,  58.05      ,  44.7       ,  58.        ,\n",
       "        34.5       ,  77.        ,  55.99      ,  26.6       ,\n",
       "        20.        ,  32.        ,  14.75      ,  37.5       ,\n",
       "        55.1       ,  22.5       ,  46.3       ,  47.85      ,\n",
       "        36.5       ,  83.4       ,  23.        ,  65.3       ,\n",
       "        27.2       ,  42.5       ,  30.8       ,  44.        ,\n",
       "        72.25      ,  22.5       ,  39.5       ,  37.8       ,\n",
       "        51.5       ,  21.4       ,  52.07145185,  26.995     ,\n",
       "       107.        ,  34.95      ,  75.25      , 195.        ,\n",
       "        28.295     ,  48.5       ,  58.28      ,  41.5       ,\n",
       "        32.85      ,  82.5       ,  69.5       ,  21.5       ,\n",
       "        54.8       ,  27.5       ,  88.5       ,  40.82      ,\n",
       "        50.5       ,  76.5       ,  72.        ,  27.        ,\n",
       "        25.235     ,  28.        ,  64.        ,  49.3       ,\n",
       "        25.5       ,  50.        ,  37.875     ,  55.        ,\n",
       "        32.3       ,  57.8       ,  36.99      ,  33.        ,\n",
       "        26.1       ,  46.95      ,  80.        ,  43.5       ,\n",
       "        64.8       , 160.        ,  20.66      ,  33.        ,\n",
       "        28.995     ,  46.5       ,  89.        ,  36.        ,\n",
       "        26.9       ,  34.75      ,  32.9       ,  18.5       ,\n",
       "        33.89      ,  22.65      ,  55.        ,  56.5       ,\n",
       "        62.5504    ,  23.        ,  38.9999    ,  60.5       ,\n",
       "        66.4       ,  43.8       , 103.        ,  32.25      ,\n",
       "        25.8       ,  54.2       ,  24.65      ,  56.        ,\n",
       "        20.795     ,  38.        ,  57.        ,  25.        ,\n",
       "        50.5       ,  29.5       ,  58.5188    ,  40.395     ,\n",
       "        56.5       ,  67.3       ,  48.5       ,  55.        ,\n",
       "        37.15      ,  63.87      ,  32.7       ,  32.9999    ,\n",
       "        76.        ,  27.4975    ,  72.75      , 213.        ,\n",
       "        30.        ,  64.5       , 225.        ,  97.        ,\n",
       "        26.7       ,  44.995     ,  86.5       ,  38.5       ,\n",
       "        44.1       , 104.        ,  65.        ,  69.11      ,\n",
       "        37.25      ,  53.1       ,  84.        ,  35.        ,\n",
       "        37.8       ,  43.5       ,  67.5       , 122.        ,\n",
       "        20.5       ,  21.        ,  50.        ,  24.        ,\n",
       "        27.9       ,  41.3       ,  24.995     , 115.        ,\n",
       "        35.        ,  71.9       ,  48.        ,  32.61      ,\n",
       "        78.5       ,  19.9       ,  65.        ,  59.5       ,\n",
       "        26.        , 120.        ,  44.5       ,  38.5       ,\n",
       "        32.5       ,  22.2       ,  49.7       ,  40.        ,\n",
       "        53.25      ,  33.8       , 110.        ,  32.5       ,\n",
       "        18.        ,  52.        ,  45.5       ,  64.245     ,\n",
       "        33.        ,  61.075     ,  22.95      ,  56.5       ,\n",
       "        45.65      ,  92.7       ,  35.95      ,  90.5       ,\n",
       "        20.        ,  28.7653    , 100.        ,  25.2       ,\n",
       "        36.        ,  34.5       ,  40.4       ,  28.84      ,\n",
       "        68.        ,  39.995     ,  86.199     ,  33.15      ,\n",
       "        45.2       ,  24.        ,  33.5       ,  35.2       ,\n",
       "        65.5       ,  43.8       ,  17.5       ,  44.        ,\n",
       "        79.9       ,  15.7       ,  79.5       ,  81.99      ,\n",
       "        45.1       ,  38.52      ,  26.        ,  70.5       ,\n",
       "        37.85      ,  39.        ,  79.        ,  23.7       ,\n",
       "        22.795     ,  90.5       ,  31.5       ,  35.8       ,\n",
       "        29.09      ,  46.        , 145.        ,  34.05      ,\n",
       "        15.3       ,  70.        ,  69.99      ,  67.5       ,\n",
       "        58.5       ,  46.8       ,  26.8       ,  66.5       ,\n",
       "        84.3       ,  20.4       ,  62.5       ,  48.5       ,\n",
       "        19.6       ,  29.9       ,  27.5       ,  95.1       ,\n",
       "        79.9       ,  26.        ,  81.1       ,  86.        ,\n",
       "        26.        ,  25.5       ,  42.        ,  43.        ,\n",
       "        52.2       ,  40.        , 109.        ,  30.6       ,\n",
       "        39.        ,  91.5       ,  36.        ,  52.5       ,\n",
       "        47.2       ,  61.        ,  47.5       ,  85.1       ,\n",
       "        66.2       ,  62.8       ,  44.2       ,  41.9       ,\n",
       "        58.5       ,  93.7       ,  26.995     ,  21.5       ,\n",
       "        52.5       ,  43.        ,  34.2       ,  48.5       ,\n",
       "        32.95      ,  79.15      ,  57.        ,  26.99      ,\n",
       "        34.69      ,  59.5       ,  25.5       ,  80.        ,\n",
       "        66.25      ,  21.        ,  44.2       ,  31.5       ,\n",
       "        62.5       ,  53.        ,  66.        ,  73.        ,\n",
       "        28.4       ,  24.9       ,  47.5       ,  82.25      ,\n",
       "        56.5       ,  21.8       ,  43.75      ,  37.9       ,\n",
       "        22.        ,  26.        ,  24.5       ,  43.        ,\n",
       "        78.        ,  47.995     ,  21.6       ,  39.        ,\n",
       "        29.        ,  26.5       ,  38.25      ,  47.7       ,\n",
       "        33.5       ,  44.        ,  32.5       ,  33.5       ,\n",
       "        28.75      ,  25.        ,  39.6       ,  51.7534    ,\n",
       "        36.1       ,  17.5       ,  61.5       ,  71.5       ,\n",
       "        74.        ,  45.9       ,  65.        , 110.        ,\n",
       "        35.        ,  70.        ,  85.99      ,  39.        ,\n",
       "        35.99      ,  28.        ,  31.45      ,  23.99      ,\n",
       "        38.5       ,  38.5       ,  80.71      ,  29.5       ,\n",
       "        83.25      ,  43.        ,  65.        ,  65.        ,\n",
       "        94.        ,  79.5       ,  24.7       ,  22.9       ,\n",
       "        31.        ,  47.5       ,  84.        ,  27.5       ,\n",
       "        32.        ,  36.495     , 139.        ,  23.        ,\n",
       "        42.        ,  62.5       ,  16.66      ,  40.        ,\n",
       "        58.1       , 225.        ,  84.995     ,  51.        ,\n",
       "        65.        ,  53.        ,  37.3       ,  52.7       ,\n",
       "        59.75      ,  28.5       ,  18.1       ,  28.        ,\n",
       "        39.15      ,  29.        ,  46.7       ,  66.4       ,\n",
       "        43.        ,  33.        ,  39.6       ,  38.5       ,\n",
       "        98.        , 200.        ,  23.3       ,  55.5       ,\n",
       "        26.35      ,  23.3       ,  40.        ,  31.5       ,\n",
       "        64.        ,  45.1       ,   8.        ,  48.1       ,\n",
       "        29.        ,  40.3       ,  26.99      ,  26.        ,\n",
       "       103.        ,  41.5       ,  46.4       ,  38.        ,\n",
       "        30.5       ,  34.8       ,  68.2       ,  37.        ,\n",
       "        16.        ,  43.5       ,  56.5       ,  46.05      ,\n",
       "       101.        ,  51.5       ,  78.2       ,  28.        ,\n",
       "        56.        ,  28.8       ,  72.5       , 238.        ,\n",
       "        53.        ,  25.5       ,  73.        ,  66.5       ,\n",
       "        30.        ,  29.        ,  37.        ,  43.55      ,\n",
       "        18.9       ,  88.        ,  74.9       ,  18.6375    ,\n",
       "        54.35      ,  50.        ,  85.        ,  74.        ,\n",
       "        30.5       ,  19.65      ,  26.85      ,  19.5       ,\n",
       "        43.8924    ,  57.1       ,  32.4       ,  40.5       ,\n",
       "       128.        ,  48.2       ,  88.        ,  25.3       ,\n",
       "        24.5       ,  32.        ,  38.7       ,  28.8349    ,\n",
       "        67.99      ,  48.495     ,  53.        ,  31.5       ,\n",
       "        30.1       ,  47.5       ,  53.36      , 131.        ,\n",
       "        47.        ,  26.495     ,  18.        ,  24.3       ,\n",
       "        55.        , 115.        ,  27.        ,  20.1       ,\n",
       "        56.        ,  66.        ,  95.        ,  17.85      ,\n",
       "        31.5       ,  24.75      ,  29.15      ,  72.        ,\n",
       "        91.75      ,  69.98      , 155.        ,  23.25      ])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "39ea8a5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 17.18  ,  23.    ,  77.5   ,  24.5   ,  27.995 ,  85.66  ,\n",
       "        24.5   ,  50.    ,  45.    ,  51.2   , 112.    ,  35.9   ,\n",
       "        68.75  ,  17.    ,  41.    ,  32.    ,  18.995 ,  20.5425,\n",
       "        20.    ,  41.995 ,  34.    ,  59.25  ,  60.4   ,  45.2   ,\n",
       "        80.2541,  24.35  ,  55.07  ,  42.212 ,  24.5   ,  59.    ,\n",
       "        97.5   ,  21.    ,  21.5   ,  23.4   ,  63.    , 122.    ,\n",
       "       120.    ,  29.7   ,  24.    ,  69.6   ,  27.425 ,  20.    ,\n",
       "        58.8   ,  61.5   ,  29.9   ,  20.4   ,  60.5   ,  38.1156,\n",
       "        33.9   ,  42.5   ,  59.25  ,  71.8   ,  95.    ,  19.995 ,\n",
       "        44.5   ,  51.995 ,  78.    ,  32.25  ,  36.55  ,  30.    ,\n",
       "       127.    ,  22.19  ,  55.    ,  40.5   ,  23.2   ,  45.    ,\n",
       "        53.    ,  22.    ,  30.5   ,  38.5   ,  32.    ,  72.95  ,\n",
       "        27.85  ,  29.99  ,  31.495 ,  25.4   , 100.    ,  43.75  ,\n",
       "        46.995 ,  39.75  ,  51.65  ,  65.    ,  63.2925,  32.5   ,\n",
       "        33.    ,  41.    ,  19.    , 145.    ,  34.15  ,  26.    ,\n",
       "        78.5   ,  39.    ,  41.9   ,  21.3   ,  69.    ,  42.    ,\n",
       "        28.2   ,  37.495 ,  65.    ,  48.7   ,  53.8   , 240.    ,\n",
       "        41.    ,  58.5   ,  56.5   ,  34.5   ,  37.4   ,  27.    ,\n",
       "        52.77  ,  47.    ,  32.5   ,  92.15  ,  43.5   ,  74.8   ,\n",
       "        21.    ,  91.    ,  20.8633,  79.9   , 137.    ,  41.6   ,\n",
       "        43.7   , 130.    ,  78.4   ,  26.75  ,  65.    ,  53.5   ,\n",
       "        56.    , 110.    ,  22.3   ,  63.9   ,  28.95  ,  65.5   ,\n",
       "       125.    ,  52.9   ,  37.5   ,  52.5   ,  66.995 ,  52.    ,\n",
       "        34.7   ,  16.5   ,  40.53  ,  33.    ,  94.    ,  32.85  ,\n",
       "        36.3   , 307.    ,  48.    ,  42.875 ,  18.5   ,  88.8   ,\n",
       "        72.    ,  23.5   ,  70.    ,  25.995 ,  65.9   ,  43.1   ,\n",
       "        60.1   ,  42.5   ,  32.4   ,  41.5   ,  36.5   ,  36.    ,\n",
       "        67.15  ,  31.5   ,  58.9   ,  36.    , 107.    ,  42.5   ,\n",
       "        37.8   ,  22.5   ,  40.5   ,  71.    ,  24.5   ,  25.    ,\n",
       "        16.35  ,  34.25  ,  28.9   ,  51.75  ,  91.    ,  50.7   ,\n",
       "        38.75  ,  60.495 ,  42.3   ,  53.5   ,  36.995 ,  61.5   ,\n",
       "        83.5   , 240.    ,  31.095 ,  25.    ,  31.65  ,  40.    ,\n",
       "        28.7   ,  67.79  ,  32.5   ,  40.5   ,  39.5   ,  43.5   ,\n",
       "        46.7   ,  50.    , 135.    ,  59.    ,  22.8   ,  37.5   ,\n",
       "        35.6   ,  44.5838,  26.8   ,  66.25  ,  25.27  ,  46.    ,\n",
       "        22.6   , 110.    ,  29.6   ,  34.9   ,  39.7   ,  94.25  ,\n",
       "        65.    ,  64.    ,  34.    ,  45.    ,  75.    ,  31.5   ,\n",
       "        80.    ,  29.185 ,  47.    , 145.    ,  56.35  ,  86.87  ,\n",
       "        51.85  ,  43.165 ,  21.049 ,  99.    ,  52.5   ,  47.    ,\n",
       "        22.5   ,  21.8   ,  42.99  ,  78.5   ,  40.56  ,  28.5   ,\n",
       "        42.    ,  50.1   ,  18.85  , 120.    ,  90.    ,  32.3   ,\n",
       "        70.    ,  38.4   ,  42.72  ,  20.9   , 130.    ,  42.3   ,\n",
       "        53.8   ,  41.49  ,  56.    ,  49.65  ,  27.05  ,  57.    ,\n",
       "       149.    ,  46.9   ,  49.    ,  30.    ,  60.5   ,  28.5   ,\n",
       "        43.9   , 110.    ,  29.1   ,  48.    ,  32.8   ,  45.    ,\n",
       "        50.5   , 123.    ,  30.8   ,  30.5   ,  48.    , 225.    ,\n",
       "        34.2   ,  32.    ,  31.4   ,  24.5   ,  21.5   ,  42.5   ,\n",
       "        44.35  ,  64.    ,  49.    ,  31.8888,  85.083 ,  39.5   ,\n",
       "        93.    ,  98.8   ,  63.6   ,  54.5   ,  33.7   ,  35.5   ,\n",
       "        31.    ,  16.695 ,  40.495 ,  24.05  ,  30.5   ,  31.5   ])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9103af24",
   "metadata": {},
   "source": [
    "# There are a few ways to determine if a model is overfitting or underfitting:\n",
    "\n",
    "1. Validation Curves: One way to check for overfitting or underfitting is by plotting validation curves. These plots show the performance of the model on the training set and the validation set as a function of a model parameter. If the model performs well on the training set but poorly on the validation set, it is likely overfitting. If the model performs poorly on both the training set and the validation set, it is likely underfitting.\n",
    "\n",
    "2. Learning Curves: Another way to check for overfitting or underfitting is by plotting learning curves. These plots show the performance of the model on the training set and the validation set as a function of the number of training examples. If the model performs well on the training set but poorly on the validation set, it is likely overfitting. If the model performs poorly on both the training set and the validation set, it is likely underfitting.\n",
    "\n",
    "3. Residuals Plot : Checking the residuals plot, residuals should be randomly distributed around the zero line, if there is apattern in the residuals this could be an indication of underfitting.\n",
    "\n",
    "4. Cross-Validation: Another way to check for overfitting and underfitting is by using cross-validation. This is a technique where the data is split into multiple subsets and the model is trained and evaluated multiple times. If the model performs well on the training set but poorly on the validation set, it is likely overfitting. If the model performs poorly on both the training set and the validation set, it is likely underfitting.\n",
    "\n",
    "5. Comparing Training and Testing Accuracy: If the training accuracy is much higher than the testing accuracy, it's likely that the model is overfitting. If the testing accuracy is much lower than the training accuracy, it's likely that the model is underfitting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81fe6b8",
   "metadata": {},
   "source": [
    "# How can we deal with overfitting data?\n",
    ">>Using Regularization: Regularization technique is used to prevent overfitting by adding a penalty term to the loss function, which discourages large weights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e051565e",
   "metadata": {},
   "source": [
    "# Apply Algorithm SGDRegressor \n",
    ">the Stochastic Gradient Descent (SGD) algorithm. The goal of linear regression is to find the best linear relationship between a dependent variable and one or more independent variables. SGDRegressor can be used to find the best fit line/plane that minimizes the mean squared error between the predicted and actual values. \n",
    "\n",
    "## SGRRegresor parameters.\n",
    "\n",
    "penalty:- This parameter specifies the type of regularization to be applied to the model. In this case, it is set to 'l2', which corresponds to L2 regularization, also known as Ridge regularization. This will add a term to the cost function that penalizes the sum of the squares of the coefficients.\n",
    "\n",
    "max_iter:- This parameter sets the number of iterations for which the optimization algorithm will run. In this case, it is set to 1000.\n",
    "\n",
    "tol:- This parameter controls the tolerance for convergence. The optimization algorithm will stop when the relative change in the cost function is below this value. In this case, it is set to 1e-3.\n",
    "\n",
    "loss:- This parameter sets the loss function to be used. In this case, it is set to 'squared_loss', which corresponds to the mean squared error loss function, commonly used for linear regression.\n",
    "\n",
    "With this configuration, the SGDRegressor model will be regularized by L2 regularization, will run for 1000 iterations and will stop when relative change in the cost function is below 1e-3. The loss function is mean squared error.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "698b5495",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "62f5d227",
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd = SGDRegressor( penalty = 'l2' , max_iter=1000, tol=1e-3 , loss = 'squared_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "22a5981a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AHMED\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:165: FutureWarning: The loss 'squared_loss' was deprecated in v1.0 and will be removed in version 1.2. Use `loss='squared_error'` which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SGDRegressor(loss='squared_loss')"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgd.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "834b4064",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AHMED\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:165: FutureWarning: The loss 'squared_loss' was deprecated in v1.0 and will be removed in version 1.2. Use `loss='squared_error'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\AHMED\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:165: FutureWarning: The loss 'squared_loss' was deprecated in v1.0 and will be removed in version 1.2. Use `loss='squared_error'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\AHMED\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:165: FutureWarning: The loss 'squared_loss' was deprecated in v1.0 and will be removed in version 1.2. Use `loss='squared_error'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\AHMED\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:165: FutureWarning: The loss 'squared_loss' was deprecated in v1.0 and will be removed in version 1.2. Use `loss='squared_error'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\AHMED\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:165: FutureWarning: The loss 'squared_loss' was deprecated in v1.0 and will be removed in version 1.2. Use `loss='squared_error'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\AHMED\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:165: FutureWarning: The loss 'squared_loss' was deprecated in v1.0 and will be removed in version 1.2. Use `loss='squared_error'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\AHMED\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:165: FutureWarning: The loss 'squared_loss' was deprecated in v1.0 and will be removed in version 1.2. Use `loss='squared_error'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\AHMED\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:165: FutureWarning: The loss 'squared_loss' was deprecated in v1.0 and will be removed in version 1.2. Use `loss='squared_error'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\AHMED\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:165: FutureWarning: The loss 'squared_loss' was deprecated in v1.0 and will be removed in version 1.2. Use `loss='squared_error'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\AHMED\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:165: FutureWarning: The loss 'squared_loss' was deprecated in v1.0 and will be removed in version 1.2. Use `loss='squared_error'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\AHMED\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:165: FutureWarning: The loss 'squared_loss' was deprecated in v1.0 and will be removed in version 1.2. Use `loss='squared_error'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\AHMED\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:165: FutureWarning: The loss 'squared_loss' was deprecated in v1.0 and will be removed in version 1.2. Use `loss='squared_error'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\AHMED\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:165: FutureWarning: The loss 'squared_loss' was deprecated in v1.0 and will be removed in version 1.2. Use `loss='squared_error'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\AHMED\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:165: FutureWarning: The loss 'squared_loss' was deprecated in v1.0 and will be removed in version 1.2. Use `loss='squared_error'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\AHMED\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:165: FutureWarning: The loss 'squared_loss' was deprecated in v1.0 and will be removed in version 1.2. Use `loss='squared_error'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\AHMED\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:165: FutureWarning: The loss 'squared_loss' was deprecated in v1.0 and will be removed in version 1.2. Use `loss='squared_error'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\AHMED\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:165: FutureWarning: The loss 'squared_loss' was deprecated in v1.0 and will be removed in version 1.2. Use `loss='squared_error'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\AHMED\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:165: FutureWarning: The loss 'squared_loss' was deprecated in v1.0 and will be removed in version 1.2. Use `loss='squared_error'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\AHMED\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:165: FutureWarning: The loss 'squared_loss' was deprecated in v1.0 and will be removed in version 1.2. Use `loss='squared_error'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\AHMED\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:165: FutureWarning: The loss 'squared_loss' was deprecated in v1.0 and will be removed in version 1.2. Use `loss='squared_error'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\AHMED\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:165: FutureWarning: The loss 'squared_loss' was deprecated in v1.0 and will be removed in version 1.2. Use `loss='squared_error'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\AHMED\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:165: FutureWarning: The loss 'squared_loss' was deprecated in v1.0 and will be removed in version 1.2. Use `loss='squared_error'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\AHMED\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:165: FutureWarning: The loss 'squared_loss' was deprecated in v1.0 and will be removed in version 1.2. Use `loss='squared_error'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\AHMED\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:165: FutureWarning: The loss 'squared_loss' was deprecated in v1.0 and will be removed in version 1.2. Use `loss='squared_error'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\AHMED\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:165: FutureWarning: The loss 'squared_loss' was deprecated in v1.0 and will be removed in version 1.2. Use `loss='squared_error'` which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#Checking status of model\n",
    "from sklearn.model_selection import learning_curve\n",
    "train_sizes, train_scores, test_scores = learning_curve(sgd, X_train, y_train, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2a265391",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the mean and standard deviation of the training and test scores\n",
    "train_mean = train_scores.mean(axis=1)\n",
    "train_std = train_scores.std(axis=1)\n",
    "test_mean = test_scores.mean(axis=1)\n",
    "test_std = test_scores.std(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1e8618d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Plot the learning curve\n",
    "plt.plot(train_sizes, train_mean, label='Training score', color='r')\n",
    "plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, color='r', alpha=0.2)\n",
    "plt.plot(train_sizes, test_mean, label='Cross-validation score', color='g')\n",
    "plt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, color='g', alpha=0.2)\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel('Number of training examples')\n",
    "plt.ylabel('Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "609bad2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = sgd.score(X_train,y_train) #Check training data performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "dcdee575",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = sgd.score(X_test,y_test) # #Check test data performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "678f3648",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is overfitting\n"
     ]
    }
   ],
   "source": [
    "# Check for overfitting or underfitting\n",
    "if train > test:\n",
    "    print(\"Model is overfitting\")\n",
    "elif train < test:\n",
    "    print(\"Model is underfitting\")\n",
    "else:\n",
    "    print(\"Model is neither overfitting nor underfitting\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "555c7dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = sgd.predict(X_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "499a4164",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([26.10646927, 19.93670757, 63.31237566, 23.93074113, 27.12571739,\n",
       "       77.50285602, 30.39772685, 55.79905863, 38.16995606, 38.80922473])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3fc61d9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([17.18 , 23.   , 77.5  , 24.5  , 27.995, 85.66 , 24.5  , 50.   ,\n",
       "       45.   , 51.2  ])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "79da3929",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check error in model\n",
    "from sklearn.metrics import mean_absolute_error \n",
    "from sklearn.metrics import mean_squared_error \n",
    "from sklearn.metrics import median_absolute_error\n",
    "from sklearn.metrics import r2_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e5e51cd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error Value is :  12.700139062665805\n"
     ]
    }
   ],
   "source": [
    "#Calculating Mean Absolute Error\n",
    "MAEValue = mean_absolute_error(y_test, y_pred, multioutput='uniform_average') # average error of values\n",
    "print('Mean Absolute Error Value is : ', MAEValue)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "efabc009",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error Value is :  378.9914411909315\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "MSEValue = mean_squared_error(y_test, y_pred, multioutput='uniform_average') # it can be raw_values\n",
    "print('Mean Squared Error Value is : ', MSEValue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "25ae5a65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error is :  19.467702514445087\n"
     ]
    }
   ],
   "source": [
    "#Calculating Root mean Squared Error\n",
    "print('Root Mean Squared Error is : ', np.sqrt(mean_squared_error(y_test, y_pred)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b70f26ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median Squared Error Value is :  8.875896209318023\n"
     ]
    }
   ],
   "source": [
    "#Calculating Median Squared Error\n",
    "MdSEValue = median_absolute_error(y_test, y_pred)\n",
    "print('Median Squared Error Value is : ', MdSEValue )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "29d3cf21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-squared Error Value is :  0.6974556862448736\n"
     ]
    }
   ],
   "source": [
    "#Calculating R-squared Error\n",
    "from sklearn.metrics import r2_score\n",
    "RValue = r2_score(y_test, y_pred)\n",
    "print('R-squared Error Value is : ', RValue )\n",
    "#40 % to 80 % is good value "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
